{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing libraries..\n",
      "loading model..\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "print('importing libraries..')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import time\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017'\n",
    "MODEL_NAME = 'faster_rcnn_resnet101_coco_11_06_2017'\n",
    "\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "print ('loading model..')\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "print('model loaded')\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Detection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detection(path):\n",
    "    points_objs = {}\n",
    "    frame_objs = {}\n",
    "    start = time.time()\n",
    "    \n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            skip = 0\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            id_frame = 1\n",
    "            id_center = 0\n",
    "            \n",
    "            while(True):\n",
    "                ret, frame = cap.read()\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "                skip = 0\n",
    "                image_np = np.array(frame)\n",
    "                if(image_np.shape == ()): break\n",
    "\n",
    "                print('Frame ID:', id_frame, '\\tTime:', '{0:.2f}'.format(time.time()-start), 'seconds')\n",
    "                frame_objs[id_frame] = image_np\n",
    "                image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "\n",
    "                image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "                boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "                scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "                classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "                num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "                (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "                boxes = np.squeeze(boxes)\n",
    "                classes = np.squeeze(classes).astype(np.int32)\n",
    "                scores = np.squeeze(scores)\n",
    "\n",
    "                count_boxes = 0\n",
    "                thresh = 0.3\n",
    "                max_boxes = 50\n",
    "\n",
    "                for i, c in enumerate(classes):\n",
    "                    if (c == 1 and (scores[i] > thresh) and (count_boxes < max_boxes)):\n",
    "                        im_height = image_np.shape[0]\n",
    "                        im_width = image_np.shape[1]\n",
    "                        ymin, xmin, ymax, xmax = boxes[i]\n",
    "                      \n",
    "                        (left, right, top, bottom) = (int(xmin*im_width),  int(xmax*im_width),\n",
    "                                                      int(ymin*im_height), int(ymax*im_height))\n",
    "                        points_objs[id_center] = {\n",
    "                            'frame': id_frame,\n",
    "                            'bbox': [left, top, right-left, bottom-top],\n",
    "                            'score': scores[i]\n",
    "                        }\n",
    "                        count_boxes += 1\n",
    "\n",
    "                id_center += 1\n",
    "                id_frame += 1\n",
    "                \n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        return points_objs, frame_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame ID: 0 \tTime: 0.35 seconds\n",
      "Frame ID: 1 \tTime: 11.38 seconds\n",
      "Frame ID: 2 \tTime: 12.73 seconds\n",
      "Frame ID: 3 \tTime: 14.08 seconds\n",
      "Frame ID: 4 \tTime: 15.42 seconds\n",
      "Frame ID: 5 \tTime: 16.75 seconds\n",
      "Frame ID: 6 \tTime: 18.10 seconds\n",
      "Frame ID: 7 \tTime: 19.45 seconds\n",
      "Frame ID: 8 \tTime: 20.80 seconds\n",
      "Frame ID: 9 \tTime: 22.16 seconds\n",
      "Frame ID: 10 \tTime: 23.52 seconds\n",
      "Frame ID: 11 \tTime: 24.87 seconds\n",
      "Frame ID: 12 \tTime: 26.24 seconds\n",
      "Frame ID: 13 \tTime: 27.60 seconds\n",
      "Frame ID: 14 \tTime: 28.96 seconds\n",
      "Frame ID: 15 \tTime: 30.33 seconds\n",
      "Frame ID: 16 \tTime: 31.71 seconds\n",
      "Frame ID: 17 \tTime: 33.07 seconds\n",
      "Frame ID: 18 \tTime: 34.42 seconds\n",
      "Frame ID: 19 \tTime: 35.78 seconds\n",
      "Frame ID: 20 \tTime: 37.13 seconds\n",
      "Frame ID: 21 \tTime: 38.48 seconds\n",
      "Frame ID: 22 \tTime: 39.84 seconds\n",
      "Frame ID: 23 \tTime: 41.19 seconds\n",
      "Frame ID: 24 \tTime: 42.54 seconds\n",
      "Frame ID: 25 \tTime: 43.93 seconds\n",
      "Frame ID: 26 \tTime: 45.28 seconds\n",
      "Frame ID: 27 \tTime: 46.63 seconds\n",
      "Frame ID: 28 \tTime: 47.98 seconds\n",
      "Frame ID: 29 \tTime: 49.33 seconds\n",
      "Frame ID: 30 \tTime: 50.70 seconds\n",
      "Frame ID: 31 \tTime: 52.06 seconds\n",
      "Frame ID: 32 \tTime: 53.45 seconds\n",
      "Frame ID: 33 \tTime: 54.84 seconds\n",
      "Frame ID: 34 \tTime: 56.20 seconds\n",
      "Frame ID: 35 \tTime: 57.56 seconds\n",
      "Frame ID: 36 \tTime: 58.91 seconds\n",
      "Frame ID: 37 \tTime: 60.26 seconds\n",
      "Frame ID: 38 \tTime: 61.61 seconds\n",
      "Frame ID: 39 \tTime: 62.97 seconds\n",
      "Frame ID: 40 \tTime: 64.32 seconds\n",
      "Frame ID: 41 \tTime: 65.66 seconds\n",
      "Frame ID: 42 \tTime: 67.02 seconds\n",
      "Frame ID: 43 \tTime: 68.37 seconds\n",
      "Frame ID: 44 \tTime: 69.73 seconds\n",
      "Frame ID: 45 \tTime: 71.07 seconds\n",
      "Frame ID: 46 \tTime: 72.43 seconds\n",
      "Frame ID: 47 \tTime: 73.78 seconds\n",
      "Frame ID: 48 \tTime: 75.13 seconds\n",
      "Frame ID: 49 \tTime: 76.47 seconds\n",
      "Frame ID: 50 \tTime: 77.82 seconds\n",
      "Frame ID: 51 \tTime: 79.16 seconds\n",
      "Frame ID: 52 \tTime: 80.53 seconds\n",
      "Frame ID: 53 \tTime: 81.88 seconds\n",
      "Frame ID: 54 \tTime: 83.23 seconds\n",
      "Frame ID: 55 \tTime: 84.58 seconds\n",
      "Frame ID: 56 \tTime: 85.93 seconds\n",
      "Frame ID: 57 \tTime: 87.27 seconds\n",
      "Frame ID: 58 \tTime: 88.62 seconds\n",
      "Frame ID: 59 \tTime: 89.96 seconds\n",
      "Frame ID: 60 \tTime: 91.31 seconds\n",
      "Frame ID: 61 \tTime: 92.66 seconds\n",
      "Frame ID: 62 \tTime: 94.00 seconds\n",
      "Frame ID: 63 \tTime: 95.36 seconds\n",
      "Frame ID: 64 \tTime: 96.71 seconds\n",
      "Frame ID: 65 \tTime: 98.05 seconds\n",
      "Frame ID: 66 \tTime: 99.40 seconds\n",
      "Frame ID: 67 \tTime: 100.75 seconds\n"
     ]
    }
   ],
   "source": [
    "points_objs, frame_objs = detection(path='../data/1_BEST/1_BEST.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_points = pd.DataFrame.from_dict(points_objs, orient='index')\n",
    "df_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_boxes = []\n",
    "for left,top,width,height in df_points['bbox'].as_matrix():\n",
    "    df_boxes.append([left,top,width,height])\n",
    "df_boxes = pd.DataFrame.from_records(df_boxes)\n",
    "df_boxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_minus_ones = pd.DataFrame.from_records([[-1] for x in range(len(df_boxes))])\n",
    "df_minus_ones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_MOT = pd.concat([df_points['frame'], \n",
    "                    df_minus_ones, \n",
    "                    df_boxes, \n",
    "                    df_points['score'], \n",
    "                    df_minus_ones, \n",
    "                    df_minus_ones, \n",
    "                    df_minus_ones], axis=1)\n",
    "df_MOT.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_MOT.to_csv('../deep_sort/MOT15/train/PETS09-S2L1/det/det.txt', header=None, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
